{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Predict description of Python functions using Seq2Seq Model - 09/2019\n",
    "Felix Le\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project presents a simple sequence to sequence model which is used to predict the description (or summarization) of a Python function. The project is divided into three parts. The first section shows how we acquire and preprocess the input data. The second section demonstrates steps of building and training the seq2seq model. The final section evaluates the model's performance as well as suggests some approaches for improvement.\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "* I. [Pre-process Data](#preprocess)  \n",
    "    * 1. [Retrieve functions's code and docstring pairs from Python scripts](retrieve)\n",
    "          * [Understand Python Docstrings](#understand)  \n",
    "          * [Download and read raw python files](#download)  \n",
    "          * [Parse data and tokenize](#parse)  \n",
    "          * [Flatten code, docstring pairs and extract meta-data](#flatten)\n",
    "          * [Remove Duplicates](#remove)\n",
    "          * [Separate function without docstrings](#separate)\n",
    "          * [Partition code by repository to train, valid and test sets](#partition)\n",
    "          * [Output data in each set to .function/.docstring/.lineage files](#output)\n",
    
    "    * 2. [Prepare data for training model](#prepare)\n",
    "          * [Read Text From File](#read)\n",
    "          * [Reduce the size of training dataset](#reduce)\n",
    "          * [Tokenize Text](#tokenize)\n",
    "          * [Save tokenized text](#save)\n",
    "          * [Arrange data for modelling](#arrange)\n",
    "          \n",
    "* II. [Building and Training the Seq2Seq Model](#training)  \n",
    "     * [Create Seq2Seq Model](#create)\n",
    "     * [Train Seq2seq Model](#train)\n",
    "\n",
    "* III. [Evaluate the model](#evaluate) \n",
    "     * [Calculate BLEU score on test set](#calculate)\n",
    "     * [Preview some predicted output vs original output](#preview)\n",
    "     * [Save model on disk](#save_model)\n",
    "     * [Suggestions for improvement](#suggestion)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Pre-process Data <a name=\"preprocess\"></a>\n",
    "\n",
    "## 1. Retrieve functions's code and docstring pairs from Python scripts <a name=\"retrieve\"></a>\n",
    "\n",
    "### Understand Python Docstrings <a name=\"understand\"></a>\n",
    "\n",
    "Python Docstring is the documentation string which is written as a first statement in the function, method, class or module definition.They are a descriptive text written by a programmer mainly for themselves to know what the line of code or expression does. In fact, Docstrings are great sources of information for understanding the general purpose and functionality of the code rather than the comments which tends to focus on small steps. Therefore, they are definitely a useful source of data which can be used to train on the problem of predicting description of Python functions.\n",
    "\n",
    "In the first section, we try to parse raw python files into function and docstring pairs, tokenize both function and docstring into tokens, and split these pairs into a train, valid and test set. Let's start by importing some necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ast\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "import astor\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_utils import apply_parallel, flattenlist\n",
    "\n",
    "EN = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.9 :: Anaconda, Inc.\r\n"
     ]
    }
   ],
   "source": [
    "# Check version of Python\n",
    "# @@@ Type ! to run a shell command in Python code chunk\n",
    "! python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and read raw python files <a name=\"download\"></a>\n",
    "The next thing we need to do is to gather python code. There is an open dataset that Google hosts on BigQuey that has Python code from open source projects on GitHub. Fortunately, the Google Kubeflow team have hosted the raw data in the form of 10 csv files, available at the url: https://storage.googleapis.com/kubeflow-examples/code_search/raw_data/00000000000{i}.csv . Let's save time by reading these csv files into a pandas dataframe, and parse out some meta data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.2 s, sys: 22.7 s, total: 1min 9s\n",
      "Wall time: 5min 57s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>\"\"\"\\n.. py:module:: fnl.text.dictionary\\n   :s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>KivApple/mcu-info-util</td>\n",
       "      <td>mcu_info_util/linker_script.py</td>\n",
       "      <td>from six import iteritems\\n\\n\\ndef generate(op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Yelp/pyleus</td>\n",
       "      <td>examples/bandwith_monitoring/bandwith_monitori...</td>\n",
       "      <td>from __future__ import absolute_import, divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>jhuapl-boss/boss-manage</td>\n",
       "      <td>bin/bearer_token.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\n# Copyright 2016 The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>djfroofy/beatlounge</td>\n",
       "      <td>bl/orchestra/base.py</td>\n",
       "      <td>from itertools import cycle\\n\\nfrom twisted.py...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       nwo                                               path  \\\n",
       "0               fnl/libfnl                          src/fnl/nlp/dictionary.py   \n",
       "1   KivApple/mcu-info-util                     mcu_info_util/linker_script.py   \n",
       "2              Yelp/pyleus  examples/bandwith_monitoring/bandwith_monitori...   \n",
       "3  jhuapl-boss/boss-manage                                bin/bearer_token.py   \n",
       "4      djfroofy/beatlounge                               bl/orchestra/base.py   \n",
       "\n",
       "                                             content  \n",
       "0  \"\"\"\\n.. py:module:: fnl.text.dictionary\\n   :s...  \n",
       "1  from six import iteritems\\n\\n\\ndef generate(op...  \n",
       "2  from __future__ import absolute_import, divisi...  \n",
       "3  #!/usr/bin/env python3\\n\\n# Copyright 2016 The...  \n",
       "4  from itertools import cycle\\n\\nfrom twisted.py...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "df = pd.concat([pd.read_csv(f'https://storage.googleapis.com/kubeflow-examples/code_search/raw_data/00000000000{i}.csv')\n",
    "             for i in range(10)])\n",
    "\n",
    "# Split the column repo_path into 2 columns: nwo and path\n",
    "df['nwo'] = df['repo_path'].apply(lambda r: r.split()[0])\n",
    "df['path'] = df['repo_path'].apply(lambda r: r.split()[1])\n",
    "df.drop(columns=['repo_path'], inplace=True)\n",
    "df = df[['nwo', 'path', 'content']] # select interested columns only\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241664, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect shape of the raw data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data contains approximately 1.2 million distinct python code files. We separate it into 3 columns, where the first column shows the name of repository, the second column shows the path to the script, and the third column contains the content of that Python script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse data and tokenize <a name=\"parse\"></a>\n",
    "\n",
    "Our goal is to parse the python files into (code, docstrings) pairs. Fortunately, the standard library in python comes with the wonderful **ast** module which helps us extract function's code from files as well as extract docstrings.\n",
    "\n",
    "We also use **astor** library to strip the code of comments by doing a round trip of converting the code to an AST and then from AST back to code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_docstring(text):\n",
    "    \"Apply tokenization using spacy to docstrings.\"\n",
    "    tokens = EN.tokenizer(text)\n",
    "    return [token.text.lower() for token in tokens if not token.is_space]\n",
    "\n",
    "def tokenize_code(text):\n",
    "    \"A basic procedure for tokenizing code strings\"\n",
    "    return RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "\n",
    "def get_function_docstring_pairs(blob):\n",
    "    \"Extract (function/method, docstring) pairs from a given code blob\"\n",
    "    pairs = []\n",
    "    try:\n",
    "        module = ast.parse(blob)\n",
    "        classes = [node for node in module.body if isinstance(node, ast.ClassDef)]\n",
    "        functions = [node for node in module.body if isinstance(node, ast.FunctionDef)]\n",
    "\n",
    "        for _class in classes:\n",
    "            functions.extend([node for node in _class.body if isinstance(node, ast.FunctionDef)])\n",
    "            \n",
    "        for f in functions:\n",
    "            source = astor.to_source(f)\n",
    "            docstring = ast.get_docstring(f) if ast.get_docstring(f) else ''\n",
    "            function = source.replace(ast.get_docstring(f, clean=False), '') if docstring else source\n",
    "            pairs.append((f.name, f.lineno, source, ' '.join(tokenize_code(function)),\n",
    "                         ' '.join(tokenize_docstring(docstring.split('\\n\\n')[0]))\n",
    "                         ))\n",
    "    except (AssertionError, MemoryError, SyntaxError, UnicodeEncodeError): \n",
    "        pass\n",
    "    return pairs\n",
    "                \n",
    "def get_function_docstrings_pair_list(blob_list):\n",
    "    \"\"\"Apply the function `get_function_docstring_pairs` on a list of blobs\"\"\"\n",
    "    return [get_function_docstring_pairs(b) for b in blob_list]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function `apply_parallel` parses the code in parallel using process based threading. The number of `cpu_cores` can be adjusted according to system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.3 s, sys: 14.7 s, total: 38 s\n",
      "Wall time: 19min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairs = flattenlist(apply_parallel(get_function_docstrings_pair_list, df.content.tolist(), cpu_cores = 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element inside `pairs` is a list of tuple which contain the name of function, line number of source text, tokenization of code and tokenization of docstrings. If there is no docstring detected in the function, the docstring of that function will be represented as empty character ''. Below is an example of data contain inside an element of `pairs`:\n",
    "\n",
    "**function name**: native  \n",
    "**line number**: 50  \n",
    "**docstring**: Decorator to be used as Debian native test fixture  \n",
    "**code**: @classmethod  \n",
    "def native(cls, dsc=DEFAULT_NATIVE, opts=None):  \n",
    "    \"\"\"\"\"\"  \n",
    "    def wrapper(fn):  \n",
    "        @wraps(fn)  \n",
    "        def _native_repo(*args):  \n",
    "            repo = cls.import_native(dsc, opts)  \n",
    "            return fn(*args, repo=repo)  \n",
    "        return _native_repo  \n",
    "    return wrapper  \n",
    "    \n",
    "-------------------------------------------------------------------      \n",
    "\n",
    "**function name**: _import_one  \n",
    "**line number**: 94  \n",
    "**docstring**:  \n",
    "**code**: @classmethod  \n",
    "def _import_one(cls, dsc, opts):  \n",
    "    opts = opts or []  \n",
    "    assert import_dsc(['arg0'] + opts + [dsc]) == 0  \n",
    "    parsed = DscFile(dsc)  \n",
    "    return ComponentTestGitRepository(parsed.pkg)  \n",
    "\n",
    "-------------------------------------------------------------------  \n",
    "\n",
    "**function name**: import_debian_tarball  \n",
    "**line number**: 135  \n",
    "**docstring**: Import a 3.0 (quilt) debian dir for overlay mode  \n",
    "**code**: @classmethod  \n",
    "def import_debian_tarball(cls, debian=DEFAULT_OVERLAY, opts=None):  \n",
    "    \"\"\"\"\"\"  \n",
    "    repo = GitRepository.create(os.path.split('/')[-1].split('_')[0])  \n",
    "    UnpackTarArchive(debian, repo.path)()  \n",
    "    repo.add_files('.')  \n",
    "    repo.commit_files('.', msg='debian dir')  \n",
    "    expected_branches = ['master']  \n",
    "    ComponentTestBase._check_repo_state(repo, 'master', expected_branches)  \n",
    "    eq_(len(repo.get_commits()), 1)  \n",
    "    os.chdir(repo.path)  \n",
    "    return repo  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "      <th>pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>\"\"\"\\n.. py:module:: fnl.text.dictionary\\n   :s...</td>\n",
       "      <td>[(__init__, 19, def __init__(self, *leafs, **e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>KivApple/mcu-info-util</td>\n",
       "      <td>mcu_info_util/linker_script.py</td>\n",
       "      <td>from six import iteritems\\n\\n\\ndef generate(op...</td>\n",
       "      <td>[(generate, 4, def generate(options, filename=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Yelp/pyleus</td>\n",
       "      <td>examples/bandwith_monitoring/bandwith_monitori...</td>\n",
       "      <td>from __future__ import absolute_import, divisi...</td>\n",
       "      <td>[(__init__, 18, def __init__(self, size):\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>jhuapl-boss/boss-manage</td>\n",
       "      <td>bin/bearer_token.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\n# Copyright 2016 The...</td>\n",
       "      <td>[(request, 46, def request(url, params=None, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>djfroofy/beatlounge</td>\n",
       "      <td>bl/orchestra/base.py</td>\n",
       "      <td>from itertools import cycle\\n\\nfrom twisted.py...</td>\n",
       "      <td>[(schedule, 149, def schedule(time, func, args...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       nwo                                               path  \\\n",
       "0               fnl/libfnl                          src/fnl/nlp/dictionary.py   \n",
       "1   KivApple/mcu-info-util                     mcu_info_util/linker_script.py   \n",
       "2              Yelp/pyleus  examples/bandwith_monitoring/bandwith_monitori...   \n",
       "3  jhuapl-boss/boss-manage                                bin/bearer_token.py   \n",
       "4      djfroofy/beatlounge                               bl/orchestra/base.py   \n",
       "\n",
       "                                             content  \\\n",
       "0  \"\"\"\\n.. py:module:: fnl.text.dictionary\\n   :s...   \n",
       "1  from six import iteritems\\n\\n\\ndef generate(op...   \n",
       "2  from __future__ import absolute_import, divisi...   \n",
       "3  #!/usr/bin/env python3\\n\\n# Copyright 2016 The...   \n",
       "4  from itertools import cycle\\n\\nfrom twisted.py...   \n",
       "\n",
       "                                               pairs  \n",
       "0  [(__init__, 19, def __init__(self, *leafs, **e...  \n",
       "1  [(generate, 4, def generate(options, filename=...  \n",
       "2  [(__init__, 18, def __init__(self, size):\\n   ...  \n",
       "3  [(request, 46, def request(url, params=None, h...  \n",
       "4  [(schedule, 149, def schedule(time, func, args...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview data inside df afer parsed out\n",
    "assert len(pairs) == df.shape[0], f'Row count mismatch. `df` has {df.shape[0]:,} rows; \\\n",
    "`pairs` has {len(pairs):,} rows.'\n",
    "df['pairs'] = pairs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten code, docstring pairs and extract meta-data <a name=\"flatten\"></a>\n",
    "Flatten (code, docstring) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 23s, sys: 31.2 s, total: 5min 54s\n",
      "Wall time: 21min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# flatten pairs\n",
    "df = df.set_index(['nwo', 'path'])['pairs'].apply(pd.Series).stack() \n",
    "df = df.reset_index()\n",
    "df.columns = ['nwo', 'path', '_', 'pair'] # column 3 contain level info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract meta-data and format dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 59s, sys: 19.8 s, total: 4min 19s\n",
      "Wall time: 9min 52s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>function_name</th>\n",
       "      <th>lineno</th>\n",
       "      <th>original_function</th>\n",
       "      <th>function_tokens</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__init__</td>\n",
       "      <td>19</td>\n",
       "      <td>def __init__(self, *leafs, **edges):\\n    self...</td>\n",
       "      <td>def __init__ self leafs edges self edges edges...</td>\n",
       "      <td></td>\n",
       "      <td>https://github.comfnl/libfnl/blob/master/src/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__eq__</td>\n",
       "      <td>23</td>\n",
       "      <td>def __eq__(self, other):\\n    if isinstance(ot...</td>\n",
       "      <td>def __eq__ self other if isinstance other Node...</td>\n",
       "      <td></td>\n",
       "      <td>https://github.comfnl/libfnl/blob/master/src/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__repr__</td>\n",
       "      <td>29</td>\n",
       "      <td>def __repr__(self):\\n    return 'Node&lt;leafs={}...</td>\n",
       "      <td>def __repr__ self return Node leafs edges form...</td>\n",
       "      <td></td>\n",
       "      <td>https://github.comfnl/libfnl/blob/master/src/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>createOrGet</td>\n",
       "      <td>32</td>\n",
       "      <td>def createOrGet(self, token):\\n    \"\"\"\\n\\t\\tCr...</td>\n",
       "      <td>def createOrGet self token if token in self ed...</td>\n",
       "      <td>create or get the node pointed to by ` token `...</td>\n",
       "      <td>https://github.comfnl/libfnl/blob/master/src/f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>setLeaf</td>\n",
       "      <td>47</td>\n",
       "      <td>def setLeaf(self, key, order):\\n    \"\"\"\\n\\t\\tS...</td>\n",
       "      <td>def setLeaf self key order self leafs append o...</td>\n",
       "      <td>store the ` key ` as a leaf of this node at po...</td>\n",
       "      <td>https://github.comfnl/libfnl/blob/master/src/f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nwo                       path function_name  lineno  \\\n",
       "0  fnl/libfnl  src/fnl/nlp/dictionary.py      __init__      19   \n",
       "1  fnl/libfnl  src/fnl/nlp/dictionary.py        __eq__      23   \n",
       "2  fnl/libfnl  src/fnl/nlp/dictionary.py      __repr__      29   \n",
       "3  fnl/libfnl  src/fnl/nlp/dictionary.py   createOrGet      32   \n",
       "4  fnl/libfnl  src/fnl/nlp/dictionary.py       setLeaf      47   \n",
       "\n",
       "                                   original_function  \\\n",
       "0  def __init__(self, *leafs, **edges):\\n    self...   \n",
       "1  def __eq__(self, other):\\n    if isinstance(ot...   \n",
       "2  def __repr__(self):\\n    return 'Node<leafs={}...   \n",
       "3  def createOrGet(self, token):\\n    \"\"\"\\n\\t\\tCr...   \n",
       "4  def setLeaf(self, key, order):\\n    \"\"\"\\n\\t\\tS...   \n",
       "\n",
       "                                     function_tokens  \\\n",
       "0  def __init__ self leafs edges self edges edges...   \n",
       "1  def __eq__ self other if isinstance other Node...   \n",
       "2  def __repr__ self return Node leafs edges form...   \n",
       "3  def createOrGet self token if token in self ed...   \n",
       "4  def setLeaf self key order self leafs append o...   \n",
       "\n",
       "                                    docstring_tokens  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  create or get the node pointed to by ` token `...   \n",
       "4  store the ` key ` as a leaf of this node at po...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://github.comfnl/libfnl/blob/master/src/f...  \n",
       "1  https://github.comfnl/libfnl/blob/master/src/f...  \n",
       "2  https://github.comfnl/libfnl/blob/master/src/f...  \n",
       "3  https://github.comfnl/libfnl/blob/master/src/f...  \n",
       "4  https://github.comfnl/libfnl/blob/master/src/f...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df['function_name'] = df['pair'].apply(lambda p: p[0])\n",
    "df['lineno'] = df['pair'].apply(lambda p: p[1])\n",
    "df['original_function'] = df['pair'].apply(lambda p: p[2])\n",
    "df['function_tokens'] = df['pair'].apply(lambda p: p[3])\n",
    "df['docstring_tokens'] = df['pair'].apply(lambda p: p[4])\n",
    "df = df[['nwo', 'path', 'function_name','lineno', 'original_function', 'function_tokens', 'docstring_tokens']]\n",
    "# Add the column which contain info about github link to the python function\n",
    "# A Git blob (binary large object) is the object type used to store the contents of each file in a repository.\n",
    "df['url'] = df[['nwo', 'path', 'lineno']].apply(lambda x: 'https://github.com/{}/blob/master/{}#L{}'.\\\n",
    "                                                format(x[0], x[1], x[2]), axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 3s, sys: 434 ms, total: 4min 4s\n",
      "Wall time: 4min 5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>function_name</th>\n",
       "      <th>lineno</th>\n",
       "      <th>original_function</th>\n",
       "      <th>function_tokens</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__init__</td>\n",
       "      <td>19</td>\n",
       "      <td>def __init__(self, *leafs, **edges):\\n    self...</td>\n",
       "      <td>def __init__ self leafs edges self edges edges...</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__eq__</td>\n",
       "      <td>23</td>\n",
       "      <td>def __eq__(self, other):\\n    if isinstance(ot...</td>\n",
       "      <td>def __eq__ self other if isinstance other Node...</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__repr__</td>\n",
       "      <td>29</td>\n",
       "      <td>def __repr__(self):\\n    return 'Node&lt;leafs={}...</td>\n",
       "      <td>def __repr__ self return Node leafs edges form...</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>createOrGet</td>\n",
       "      <td>32</td>\n",
       "      <td>def createOrGet(self, token):\\n    \"\"\"\\n\\t\\tCr...</td>\n",
       "      <td>def createOrGet self token if token in self ed...</td>\n",
       "      <td>create or get the node pointed to by ` token `...</td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>setLeaf</td>\n",
       "      <td>47</td>\n",
       "      <td>def setLeaf(self, key, order):\\n    \"\"\"\\n\\t\\tS...</td>\n",
       "      <td>def setLeaf self key order self leafs append o...</td>\n",
       "      <td>store the ` key ` as a leaf of this node at po...</td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nwo                       path function_name  lineno  \\\n",
       "0  fnl/libfnl  src/fnl/nlp/dictionary.py      __init__      19   \n",
       "1  fnl/libfnl  src/fnl/nlp/dictionary.py        __eq__      23   \n",
       "2  fnl/libfnl  src/fnl/nlp/dictionary.py      __repr__      29   \n",
       "3  fnl/libfnl  src/fnl/nlp/dictionary.py   createOrGet      32   \n",
       "4  fnl/libfnl  src/fnl/nlp/dictionary.py       setLeaf      47   \n",
       "\n",
       "                                   original_function  \\\n",
       "0  def __init__(self, *leafs, **edges):\\n    self...   \n",
       "1  def __eq__(self, other):\\n    if isinstance(ot...   \n",
       "2  def __repr__(self):\\n    return 'Node<leafs={}...   \n",
       "3  def createOrGet(self, token):\\n    \"\"\"\\n\\t\\tCr...   \n",
       "4  def setLeaf(self, key, order):\\n    \"\"\"\\n\\t\\tS...   \n",
       "\n",
       "                                     function_tokens  \\\n",
       "0  def __init__ self leafs edges self edges edges...   \n",
       "1  def __eq__ self other if isinstance other Node...   \n",
       "2  def __repr__ self return Node leafs edges form...   \n",
       "3  def createOrGet self token if token in self ed...   \n",
       "4  def setLeaf self key order self leafs append o...   \n",
       "\n",
       "                                    docstring_tokens  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  create or get the node pointed to by ` token `...   \n",
       "4  store the ` key ` as a leaf of this node at po...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "1  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "2  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "3  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "4  https://github.com/fnl/libfnl/blob/master/src/...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview data inside df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates <a name=\"remove\"></a>\n",
    "\n",
    "Remove observations where the same function appears more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1,199,426 duplicate rows\n",
      "Number of functions in df after remove duplicates: 5413932\n",
      "CPU times: user 19.2 s, sys: 352 ms, total: 19.5 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "before_dedup = len(df)\n",
    "df = df.drop_duplicates(['original_function', 'function_tokens'])\n",
    "after_dedup = len(df)\n",
    "\n",
    "print(f'Removed {before_dedup - after_dedup:,} duplicate rows')\n",
    "print(f'Number of functions in df after remove duplicates: {df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate function without docstrings <a name=\"separate\"></a>\n",
    "Extract the functions without docstring and keep them in a separated dataframe. Noted that docstring should contain at least 3 words in order to be considered as a valid docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fucntions without docstrings: 4008693\n",
      "Number of functions with docstrings: 1405239\n"
     ]
    }
   ],
   "source": [
    "# function return length of list, if not a list then return 0\n",
    "def listlen(x):\n",
    "    if not isinstance(x, list):\n",
    "        return 0\n",
    "    return len(x)\n",
    "\n",
    "# Separate functions w/o docstrings\n",
    "with_docstrings = df[df.docstring_tokens.str.split().apply(listlen) >=3]\n",
    "without_docstrings = df[df.docstring_tokens.str.split().apply(listlen) < 3]\n",
    "\n",
    "print(f'Number of fucntions without docstrings: {without_docstrings.shape[0]}')\n",
    "print(f'Number of functions with docstrings: {with_docstrings.shape[0]}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition code by repository to train, valid and test sets <a name=\"partition\"></a>\n",
    "\n",
    "Each repository may have its' own coding style. Therefore we want to avoid having code from the same repository in the training set as well as in the validation and test set. This is done by grouping the functions with docstrings by their repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = with_docstrings.groupby('nwo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test set in which train set contain 87% of data\n",
    "# random_state is the seed used by the random number generator\n",
    "train, test = train_test_split(list(grouped), train_size = 0.87, shuffle=True, random_state=8801)\n",
    "\n",
    "# Split the train set further into train and validation set in which train set contain 82% of data\n",
    "train, valid = train_test_split(train, train_size=0.82, random_state=8801)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train set is a list of tuple, where each tuple contains the repository name, and the data of functions inside that repository. We will convert them to dataframe as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train, valid, test setfrom list to dataframe\n",
    "train = pd.concat([d for _, d in train]).reset_index(drop=True) \n",
    "valid = pd.concat([d for _, d in valid]).reset_index(drop=True)\n",
    "test = pd.concat([d for _, d in test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 985811\n",
      "Number of rows in validation set: 236102\n",
      "Number of rows in test set: 183326\n",
      "Without docstring rows: 4008693\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of rows in training set: {train.shape[0]}')\n",
    "print(f'Number of rows in validation set: {valid.shape[0]}')\n",
    "print(f'Number of rows in test set: {test.shape[0]}')\n",
    "print(f'Without docstring rows: {without_docstrings.shape[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview what the training set looks like. The functions token and docstring tokens are input to the models. The other information is important for diagnostics and reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>function_name</th>\n",
       "      <th>lineno</th>\n",
       "      <th>original_function</th>\n",
       "      <th>function_tokens</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>jkahn/samyro</td>\n",
       "      <td>samyro/integerize.py</td>\n",
       "      <td>reshape_cleavable</td>\n",
       "      <td>23</td>\n",
       "      <td>def reshape_cleavable(tensor, per_example_leng...</td>\n",
       "      <td>def reshape_cleavable tensor per_example_lengt...</td>\n",
       "      <td>reshapes input so that it is appropriate for s...</td>\n",
       "      <td>https://github.com/jkahn/samyro/blob/master/sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>jkahn/samyro</td>\n",
       "      <td>samyro/write.py</td>\n",
       "      <td>write</td>\n",
       "      <td>14</td>\n",
       "      <td>def write(input_placeholder, output_activation...</td>\n",
       "      <td>def write input_placeholder output_activations...</td>\n",
       "      <td>samples from the model</td>\n",
       "      <td>https://github.com/jkahn/samyro/blob/master/sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>jkahn/samyro</td>\n",
       "      <td>samyro/read.py</td>\n",
       "      <td>to_numpy_batch</td>\n",
       "      <td>23</td>\n",
       "      <td>@classmethod\\ndef to_numpy_batch(cls, batch, d...</td>\n",
       "      <td>classmethod def to_numpy_batch cls batch dtype...</td>\n",
       "      <td>takes an iterable of sample objects ; produces...</td>\n",
       "      <td>https://github.com/jkahn/samyro/blob/master/sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>jkahn/samyro</td>\n",
       "      <td>samyro/read.py</td>\n",
       "      <td>to_numpy_arrays</td>\n",
       "      <td>33</td>\n",
       "      <td>def to_numpy_arrays(self, dtype=numpy.int32):\\...</td>\n",
       "      <td>def to_numpy_arrays self dtype numpy int32 ret...</td>\n",
       "      <td>returns numpy - ified version of string inputs...</td>\n",
       "      <td>https://github.com/jkahn/samyro/blob/master/sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>jkahn/samyro</td>\n",
       "      <td>samyro/read.py</td>\n",
       "      <td>__init__</td>\n",
       "      <td>112</td>\n",
       "      <td>def __init__(self, sample_length, batch_size, ...</td>\n",
       "      <td>def __init__ self sample_length batch_size enc...</td>\n",
       "      <td>provide one of filenames or filehandles .</td>\n",
       "      <td>https://github.com/jkahn/samyro/blob/master/sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            nwo                  path      function_name  lineno  \\\n",
       "0  jkahn/samyro  samyro/integerize.py  reshape_cleavable      23   \n",
       "1  jkahn/samyro       samyro/write.py              write      14   \n",
       "2  jkahn/samyro        samyro/read.py     to_numpy_batch      23   \n",
       "3  jkahn/samyro        samyro/read.py    to_numpy_arrays      33   \n",
       "4  jkahn/samyro        samyro/read.py           __init__     112   \n",
       "\n",
       "                                   original_function  \\\n",
       "0  def reshape_cleavable(tensor, per_example_leng...   \n",
       "1  def write(input_placeholder, output_activation...   \n",
       "2  @classmethod\\ndef to_numpy_batch(cls, batch, d...   \n",
       "3  def to_numpy_arrays(self, dtype=numpy.int32):\\...   \n",
       "4  def __init__(self, sample_length, batch_size, ...   \n",
       "\n",
       "                                     function_tokens  \\\n",
       "0  def reshape_cleavable tensor per_example_lengt...   \n",
       "1  def write input_placeholder output_activations...   \n",
       "2  classmethod def to_numpy_batch cls batch dtype...   \n",
       "3  def to_numpy_arrays self dtype numpy int32 ret...   \n",
       "4  def __init__ self sample_length batch_size enc...   \n",
       "\n",
       "                                    docstring_tokens  \\\n",
       "0  reshapes input so that it is appropriate for s...   \n",
       "1                             samples from the model   \n",
       "2  takes an iterable of sample objects ; produces...   \n",
       "3  returns numpy - ified version of string inputs...   \n",
       "4          provide one of filenames or filehandles .   \n",
       "\n",
       "                                                 url  \n",
       "0  https://github.com/jkahn/samyro/blob/master/sa...  \n",
       "1  https://github.com/jkahn/samyro/blob/master/sa...  \n",
       "2  https://github.com/jkahn/samyro/blob/master/sa...  \n",
       "3  https://github.com/jkahn/samyro/blob/master/sa...  \n",
       "4  https://github.com/jkahn/samyro/blob/master/sa...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output data in each set to .function/.docstring/.lineage files <a name=\"output\"></a>\n",
    "\n",
    "The data in each train/valid/test set are written to files in the root of the `./data/processed_data` as following:\n",
    "\n",
    "   1.*{train/valid/test.function}* - these are python function definitions tokenized (by space), 1 line per function.  \n",
    "   2.*{train/valid/test.docstring}* - these are docstrings that correspond to each of the python function definitions, and have a 1:1 correspondence with the lines in *.function files.  \n",
    "   3.*{train/valid/test.lineage}* - every line in this file contains a link back to the original location (github repo link) where the code was retrieved. There is a 1:1 correspondence with the lines in this file and the other two files. This is useful for debugging.\n",
    "\n",
    "Original functions are written to compressed `json` files. (Raw functions contain ,.\\t \\n etc... It is less error-prone using json format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to(df, filename, path='./data/processed_data/'):\n",
    "    \"\"\"\n",
    "    Helper function to write processed files to disk\n",
    "    \"\"\"\n",
    "    out = Path(path)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    df.function_tokens.to_csv(out/'{}.function'.format(filename), index=False)\n",
    "    df.original_function.to_json(out/'{}_original_function.json.gz'.format(filename),\\\n",
    "                                  orient='values', compression = 'gzip')\n",
    "    if filename != 'without_docstrings':\n",
    "        df.docstring_tokens.to_csv(out/'{}.docstring'.format(filename), index=False)\n",
    "    df.url.to_csv(out/'{}.lineage'.format(filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/envs/code_description/lib/python3.6/site-packages/ipykernel_launcher.py:7: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  import sys\n",
      "/home/felix/anaconda3/envs/code_description/lib/python3.6/site-packages/ipykernel_launcher.py:11: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/felix/anaconda3/envs/code_description/lib/python3.6/site-packages/ipykernel_launcher.py:12: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Write data to output files\n",
    "write_to(train, 'train')\n",
    "write_to(valid, 'valid')\n",
    "write_to(test, 'test')\n",
    "write_to(without_docstrings, 'without_docstrings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the files that have been written in the `processed_data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.6G\r\n",
      "drwxrwxr-x 2 felix felix 4.0K Sep 13 15:17 .\r\n",
      "drwxrwxr-x 3 felix felix 4.0K Sep 13 15:11 ..\r\n",
      "-rw-rw-r-- 1 felix felix  13M Sep 13 15:14 test.docstring\r\n",
      "-rw-rw-r-- 1 felix felix  57M Sep 13 15:14 test.function\r\n",
      "-rw-rw-r-- 1 felix felix  16M Sep 13 15:14 test.lineage\r\n",
      "-rw-rw-r-- 1 felix felix  26M Sep 13 15:14 test_original_function.json.gz\r\n",
      "-rw-rw-r-- 1 felix felix  70M Sep 13 15:13 train.docstring\r\n",
      "-rw-rw-r-- 1 felix felix 306M Sep 13 15:12 train.function\r\n",
      "-rw-rw-r-- 1 felix felix  85M Sep 13 15:13 train.lineage\r\n",
      "-rw-rw-r-- 1 felix felix 138M Sep 13 15:13 train_original_function.json.gz\r\n",
      "-rw-rw-r-- 1 felix felix  19M Sep 13 15:14 valid.docstring\r\n",
      "-rw-rw-r-- 1 felix felix  70M Sep 13 15:13 valid.function\r\n",
      "-rw-rw-r-- 1 felix felix  22M Sep 13 15:14 valid.lineage\r\n",
      "-rw-rw-r-- 1 felix felix  32M Sep 13 15:14 valid_original_function.json.gz\r\n",
      "-rw-rw-r-- 1 felix felix 1.1G Sep 13 15:14 without_docstrings.function\r\n",
      "-rw-rw-r-- 1 felix felix 345M Sep 13 15:17 without_docstrings.lineage\r\n",
      "-rw-rw-r-- 1 felix felix 357M Sep 13 15:17 without_docstrings_original_function.json.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ./data/processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data for training model <a name=\"prepare\"></a>\n",
    "\n",
    "After the pairs of functions and docstring have been extracted, they need to be further processed in order to feed into the training model session later.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Text From File <a name=\"read\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from data_utils import get_step2_prerequisite_files, read_training_files\n",
    "from keras.utils import get_file\n",
    "OUTPUT_PATH = Path('./data/seq2seq/')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Num rows for encoder training + validation input: 1,221,913\n",
      "WARNING:root:Num rows for encoder holdout input: 183,326\n",
      "WARNING:root:Num rows for decoder training + validation input: 1,221,913\n",
      "WARNING:root:Num rows for decoder holdout input: 183,326\n"
     ]
    }
   ],
   "source": [
    "# Provide the directory where the files are saved before\n",
    "train_function, test_function, train_docstring, test_docstring = read_training_files('./data/processed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions-docstring pairs for training + validation input: 1221913\n",
      "Number of functions-docstring pairs for testing input: 183326\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of functions-docstring pairs for training + validation input: {len(train_function)}')\n",
    "print(f'Number of functions-docstring pairs for testing input: {len(test_docstring)}')\n",
    "\n",
    "# Make sure code and comment files have the same length\n",
    "assert len(train_function) == len(train_docstring)\n",
    "assert len(test_function) == len(test_docstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the size of training dataset <a name=\"reduce\"></a>\n",
    "\n",
    "There are about 1,2 million rows in the training set. It would take around 10 hours to train this whole dataset for one epoch using 8 CPUs. Since the project is mainly for demonstration an approach to predict code description,  we will reduce the size of training set in order to shorten the running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of functions-docstring pairs for training + validation input after minimized: 305478\n",
      "Number of functions-docstring pairs for testing input after minimized: 45831\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "reduced_factor = 4\n",
    "train_function_minimized = train_function[0:floor(len(train_function)/reduced_factor)]\n",
    "train_docstring_minimized = train_docstring[0:floor(len(train_docstring)/reduced_factor)]\n",
    "\n",
    "test_function_minimized = test_function[0:floor(len(test_function)/reduced_factor)]\n",
    "test_docstring_minimized = test_docstring[0:floor(len(test_docstring)/reduced_factor)]\n",
    "\n",
    "\n",
    "print(f'Number of functions-docstring pairs for training + validation input after minimized: {len(train_function_minimized)}')\n",
    "print(f'Number of functions-docstring pairs for testing input after minimized: {len(test_docstring_minimized)}')\n",
    "\n",
    "# Make sure code and comment files have the same length\n",
    "assert len(train_function_minimized) == len(train_docstring_minimized)\n",
    "assert len(test_function_minimized) == len(test_docstring_minimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text <a name=\"tokenize\"></a>\n",
    "In this step, we are going to pre-process the raw text for modelling. Specifically, we will clean, tokenize, apply padding or truncation to the training data by leveraging `processor` class from `ktext` library. This library is useful because it performs text processing using process-based threading in parallel, which helps to save a significant amount of running time.\n",
    "\n",
    "You can find more information about `ktext` in this [link](https://github.com/hamelsmu/ktext). Below is a short introduction about utility functions offered by this library:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Utilities for pre-processing text for deep learning in Keras.**\n",
    "\n",
    "`ktext` performs common pre-processing steps associated with deep learning (cleaning, tokenization, padding, truncation). Most importantly, `ktext` allows you to perform these steps using process-based threading in parallel. If you don't think you might benefit from parallelization, consider using the text preprocessing utilities in keras instead.\n",
    "\n",
    "`ktext` helps you with the following:\n",
    "\n",
    "1. **Cleaning** You may want to clean your data to remove items like phone numbers and email addresses and replace them with generic tags, or remove HTML. This step is optional, but can help remove noise in your data.\n",
    "\n",
    "2. **Tokenization** Take a raw string, ex \"Hello World!\" and tokenize it so it looks like ['Hello', 'World', '!']\n",
    "\n",
    "3. **Generating Vocabulary and a {Token -> index} mapping** Map each unique token in your corpus to an integer value. This usually stored as a dictionary. For example {'Hello': 2, 'World':3, '!':4} might be a valid mapping from tokens to integers. You usually want to reserve an integer for rare or unseen words (ktext uses 1) and another integer for padding (ktext uses 0). You can set a threshold for rare words (see documentation).\n",
    "\n",
    "4. **Truncating and Padding** While it is not necessary, it can be much easier if all your documents are the same length. The way we can accomplish this is through truncating and padding. For all documents below the desired length we can pad the document with 0's and documents above the desired length can be truncated. This utility allows you to build a histogram of your document lengths and choose a sensible document length for your corpus.\n",
    "\n",
    "This utility accomplishes all of the above using process-based threading for speed. Sklearn style fit, transform, and fit_transform interfaces are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 55 based upon heuristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 53 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 5 sec\n",
      "WARNING:root:Finished parsing 305,478 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 4 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 15 based upon heuristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 16 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 1 sec\n",
      "WARNING:root:Finished parsing 305,478 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 1 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 1.21 s, total: 30.3 s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from ktext.preprocess import processor\n",
    "\n",
    "# clean, tokenize, and apply padding to traing data\n",
    "function_proc = processor(heuristic_pct_padding=.7, keep_n=20000)\n",
    "t_function = function_proc.fit_transform(train_function_minimized)\n",
    "\n",
    "# append_indicators = True appends the tokens '_start_' and '_end_' to each document\n",
    "docstring_proc = processor(append_indicators=True, heuristic_pct_padding=0.7, keep_n=14000, padding='post')\n",
    "t_docstring = docstring_proc.fit_transform(train_docstring_minimized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save tokenized text <a name=\"save\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the processor\n",
    "with open(OUTPUT_PATH/'py_function_proc_v2.dpkl', 'wb') as f:\n",
    "    dpickle.dump(function_proc, f)\n",
    "with open(OUTPUT_PATH/'py_docstring_proc_v2.dpkl', 'wb') as f:\n",
    "    dpickle.dump(docstring_proc, f)\n",
    "\n",
    "# Save the processed data \n",
    "np.save(OUTPUT_PATH/'py_t_function_vecs_v2.npy', t_function)\n",
    "np.save(OUTPUT_PATH/'py_t_docstring_vecs_v2.npy', t_docstring)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrange data for modelling <a name=\"arrange\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (305478, 55)\n",
      "Shape of decoder input: (305478, 14)\n",
      "Shape of decoder target: (305478, 14)\n",
      "Size of vocabulary for data/seq2seq/py_function_proc_v2.dpkl: 20,002\n",
      "Size of vocabulary for data/seq2seq/py_docstring_proc_v2.dpkl: 14,002\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
    "\n",
    "encoder_input_data, encoder_seq_len = load_encoder_inputs(OUTPUT_PATH/'py_t_function_vecs_v2.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs(OUTPUT_PATH/'py_t_docstring_vecs_v2.npy')\n",
    "\n",
    "num_encoder_tokens, enc_pp = load_text_processor(OUTPUT_PATH/'py_function_proc_v2.dpkl')\n",
    "num_decoder_tokens, dec_pp = load_text_processor(OUTPUT_PATH/'py_docstring_proc_v2.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# II. Training Model <a name=\"training\"></a>\n",
    "\n",
    "In this part, we build the seq2seq model and train it using the data which have been prepared in part I.\n",
    "\n",
    "## Create Seq2Seq Model <a name=\"create\"></a>\n",
    "\n",
    "The convenience function `build_seq2seq_model` constructs the architecture for a sequence-to-sequence model.\n",
    "In this project, we build a simple architecture with only one layer for the encoder and decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/felix/anaconda3/envs/code_description/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/felix/anaconda3/envs/code_description/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/felix/anaconda3/envs/code_description/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/felix/anaconda3/envs/code_description/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import build_seq2seq_model\n",
    "\n",
    "seq2seq_Model = build_seq2seq_model(word_emb_dim=800,\n",
    "                                   hidden_state_dim=1000,\n",
    "                                   encoder_seq_len=encoder_seq_len,\n",
    "                                   num_encoder_tokens=num_encoder_tokens,\n",
    "                                   num_decoder_tokens=num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 800)    11201600    Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 1000)         21407800    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 1000), 5403000     Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 1000)   4000        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 14002)  14016002    Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 52,035,602\n",
      "Trainable params: 52,030,402\n",
      "Non-trainable params: 5,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 437.50 410.00\" width=\"438pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-406 433.5,-406 433.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139966189921280 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139966189921280</title>\n",
       "<polygon fill=\"none\" points=\"47,-365.5 47,-401.5 210,-401.5 210,-365.5 47,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128.5\" y=\"-379.8\">Decoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139966189921616 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139966189921616</title>\n",
       "<polygon fill=\"none\" points=\"10.5,-292.5 10.5,-328.5 246.5,-328.5 246.5,-292.5 10.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128.5\" y=\"-306.8\">Decoder-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 139966189921280&#45;&gt;139966189921616 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139966189921280-&gt;139966189921616</title>\n",
       "<path d=\"M128.5,-365.4551C128.5,-357.3828 128.5,-347.6764 128.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"132.0001,-338.5903 128.5,-328.5904 125.0001,-338.5904 132.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139966189630240 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139966189630240</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 257,-255.5 257,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"128.5\" y=\"-233.8\">Decoder-Batchnorm-1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139966189921616&#45;&gt;139966189630240 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139966189921616-&gt;139966189630240</title>\n",
       "<path d=\"M128.5,-292.4551C128.5,-284.3828 128.5,-274.6764 128.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"132.0001,-265.5903 128.5,-255.5904 125.0001,-265.5904 132.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139966205496008 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139966205496008</title>\n",
       "<polygon fill=\"none\" points=\"267.5,-292.5 267.5,-328.5 429.5,-328.5 429.5,-292.5 267.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-306.8\">Encoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139966189928176 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139966189928176</title>\n",
       "<polygon fill=\"none\" points=\"275.5,-219.5 275.5,-255.5 421.5,-255.5 421.5,-219.5 275.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"348.5\" y=\"-233.8\">Encoder-Model: Model</text>\n",
       "</g>\n",
       "<!-- 139966205496008&#45;&gt;139966189928176 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139966205496008-&gt;139966189928176</title>\n",
       "<path d=\"M348.5,-292.4551C348.5,-284.3828 348.5,-274.6764 348.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"352.0001,-265.5903 348.5,-255.5904 345.0001,-265.5904 352.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139966189339200 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>139966189339200</title>\n",
       "<polygon fill=\"none\" points=\"173,-146.5 173,-182.5 304,-182.5 304,-146.5 173,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.5\" y=\"-160.8\">Decoder-GRU: GRU</text>\n",
       "</g>\n",
       "<!-- 139966189630240&#45;&gt;139966189339200 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139966189630240-&gt;139966189339200</title>\n",
       "<path d=\"M155.691,-219.4551C169.8378,-210.0667 187.3157,-198.4678 202.563,-188.3491\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"204.8437,-191.0362 211.2406,-182.5904 200.9731,-185.2037 204.8437,-191.0362\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139966189928176&#45;&gt;139966189339200 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>139966189928176-&gt;139966189339200</title>\n",
       "<path d=\"M321.309,-219.4551C307.1622,-210.0667 289.6843,-198.4678 274.437,-188.3491\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"276.0269,-185.2037 265.7594,-182.5904 272.1563,-191.0362 276.0269,-185.2037\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139966189371232 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>139966189371232</title>\n",
       "<polygon fill=\"none\" points=\"110,-73.5 110,-109.5 367,-109.5 367,-73.5 110,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.5\" y=\"-87.8\">Decoder-Batchnorm-2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 139966189339200&#45;&gt;139966189371232 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>139966189339200-&gt;139966189371232</title>\n",
       "<path d=\"M238.5,-146.4551C238.5,-138.3828 238.5,-128.6764 238.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"242.0001,-119.5903 238.5,-109.5904 235.0001,-119.5904 242.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139966189377688 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>139966189377688</title>\n",
       "<polygon fill=\"none\" points=\"155.5,-.5 155.5,-36.5 321.5,-36.5 321.5,-.5 155.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"238.5\" y=\"-14.8\">Final-Output-Dense: Dense</text>\n",
       "</g>\n",
       "<!-- 139966189371232&#45;&gt;139966189377688 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>139966189371232-&gt;139966189377688</title>\n",
       "<path d=\"M238.5,-73.4551C238.5,-65.3828 238.5,-55.6764 238.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"242.0001,-46.5903 238.5,-36.5904 235.0001,-46.5904 242.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View architecture of the model\n",
    "from seq2seq_utils import viz_model_architecture\n",
    "viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Seq2seq Model <a name=\"train\"></a>\n",
    "\n",
    "The model is trained with 4 epochs using 8 CPUs, which takes around 8 hours to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/felix/anaconda3/envs/code_description/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/felix/anaconda3/envs/code_description/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/felix/anaconda3/envs/code_description/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/felix/anaconda3/envs/code_description/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 268820 samples, validate on 36658 samples\n",
      "Epoch 1/4\n",
      "268820/268820 [==============================] - 6996s 26ms/step - loss: 5.0330 - val_loss: 4.0671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/envs/code_description/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 1000) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4\n",
      "268820/268820 [==============================] - 7000s 26ms/step - loss: 3.8863 - val_loss: 3.8175\n",
      "Epoch 3/4\n",
      "268820/268820 [==============================] - 6957s 26ms/step - loss: 3.6533 - val_loss: 3.6546\n",
      "Epoch 4/4\n",
      "268820/268820 [==============================] - 6964s 26ms/step - loss: 3.4713 - val_loss: 3.5176\n",
      "CPU times: user 23h 16min 57s, sys: 10min 9s, total: 23h 27min 6s\n",
      "Wall time: 7h 45min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy')\n",
    "script_name_base = 'py_func_sum_v9_'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                                                                           save_best_only=True)\n",
    "batch_size = 1100\n",
    "epochs = 4\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Evaluate Model <a name=\"evaluate\"></a>\n",
    "\n",
    "## Calculate BLEU score on test set <a name=\"calculate\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f81f232b2e4deca2b24d7aec05d710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=45831), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Calculating BLEU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01936431480645111"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will return a BLEU Score\n",
    "seq2seq_inf.evaluate_model(input_strings=test_function_minimized, \n",
    "                           output_strings=test_docstring_minimized, \n",
    "                           max_len=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview some predicted output vs original output <a name=\"preview\"></a>\n",
    "\n",
    "The code below select randomly 15 examples and display their predicted values from the test set. In each example, *Original Input* is the function tokens, *Orignal Output* is the original docstring and *Predicted Output* is the predicted docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 8103 =================\n",
      "\n",
      "Original Input:\n",
      " def _test_path self path strings with app test_request_context login self app test example com test response self app get path follow_redirects True self assertEqual response status_code 200 for string in strings self assertIn string response data headers_keys key lower for key in response headers keys self assertIn etag headers_keys etag response headers get etag self assertNotEqual etag response self app get path headers If None Match etag follow_redirects True self assertEqual response status_code 304 self assertEqual response data\n",
      " \n",
      "\n",
      "Original Output:\n",
      " helper function . test response for 200 status code . also test if response body contains event / service name .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test that the user has a valid file\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 34713 =================\n",
      "\n",
      "Original Input:\n",
      " def delete_permission self name currentsession None try if currentsession session currentsession else session self create_session p self get_permission name currentsession session session delete p session commit if not currentsession session close return True except NoResultFound return False\n",
      " \n",
      "\n",
      "Original Output:\n",
      " delete the permission by name . @type name : string @param name : the name of the permission @rtype : boolean @return : true in case of success\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " delete a new instance\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 25819 =================\n",
      "\n",
      "Original Input:\n",
      " def __init__ self super SearchPublishManager self __init__\n",
      " \n",
      "\n",
      "Original Output:\n",
      " inits searchpublishmanager .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " initialize the class\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 6373 =================\n",
      "\n",
      "Original Input:\n",
      " def length_of_payment b 1000 0 p 100 0 apr 0 18 i apr 30 0 return 1 0 30 0 np log 1 0 b p 1 0 np power 1 0 i 30 0 np log 1 0 i\n",
      " \n",
      "\n",
      "Original Output:\n",
      " calculate the length of payments of b balance with p payment at apr apr .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return the number of the number of the number of the number of the number\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 14327 =================\n",
      "\n",
      "Original Input:\n",
      " def getRobotPosition self return self robotPos\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"returns the ( x , y ) coordinates of the lower - left point of the robot\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the current value of the current time\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 27880 =================\n",
      "\n",
      "Original Input:\n",
      " def current_code_list f sys _getframe f_back code_list while f code_list append f f_code code_list append f f_lineno f f f_back return code_list\n",
      " \n",
      "\n",
      "Original Output:\n",
      " returns a code - list that can be formatted by code_list2trace_list\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a list of all the given value\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 10685 =================\n",
      "\n",
      "Original Input:\n",
      " def get_tag_names self root etree fromstring self xml_full_text encode utf 8 return self get_children_tag_names root\n",
      " \n",
      "\n",
      "Original Output:\n",
      " returns the set of tag names present in the xml .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the list of the given path\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 1543 =================\n",
      "\n",
      "Original Input:\n",
      " def clear self model args d key0 self lru model if args _logger warn ormcache clear arguments are deprecated and ignored while clearing caches on s s model _name self method __name__ d clear_prefix key0 model pool _any_cache_cleared True\n",
      " \n",
      "\n",
      "Original Output:\n",
      " remove * args entry from the cache or all keys if * args is undefined\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " clear the data from the given file\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 27984 =================\n",
      "\n",
      "Original Input:\n",
      " def disable_asyncore_loop self self _timer None\n",
      " \n",
      "\n",
      "Original Output:\n",
      " removes our asyncore loop from qt 's event queue .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " called when the current user is called when the given is not already\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 37380 =================\n",
      "\n",
      "Original Input:\n",
      " def entropy_pmf pmf pmf np asarray pmf return np nansum pmf np log2 pmf axis 1\n",
      " \n",
      "\n",
      "Original Output:\n",
      " returns the entropy of the probability mass function .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the number of the given data\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 40507 =================\n",
      "\n",
      "Original Input:\n",
      " property def unit_price self unit self unit_price_excl_tax tax unit self tax_rate rate p prices Price settings ACCOUNTING_DEFAULT_CURRENCY unit tax tax return p\n",
      " \n",
      "\n",
      "Original Output:\n",
      " returns the ` price ` instance representing the instance\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return the current state of the current playing media\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 10212 =================\n",
      "\n",
      "Original Input:\n",
      " def session_start_pb hparams model_uri monitor_url group_name start_time_secs None if start_time_secs is None start_time_secs time time session_start_info plugin_data_pb2 SessionStartInfo model_uri model_uri monitor_url monitor_url group_name group_name start_time_secs start_time_secs for hp_name hp_val in six iteritems hparams session_start_info hparams hp_name CopyFrom _to_google_protobuf_value hp_val return _summary metadata SESSION_START_INFO_TAG plugin_data_pb2 HParamsPluginData session_start_info session_start_info\n",
      " \n",
      "\n",
      "Original Output:\n",
      " creates a summary that contains a training session metadata information . one such summary per training session should be created . each should have a different run .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return the current user\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 33994 =================\n",
      "\n",
      "Original Input:\n",
      " def decodeFile self path self sourcePath text if path is None return if not isVideo path self popupMessage File The input file does not seem to be a video file return input_res 640x480 cmd ffmpeg i s path cmd r 1 f rawvideo lvdodec s input_res q 6 qmin 1 qmax 4 cmd cmd self destinationPath text bin_directory os getcwd os path sep bin p Popen cmd stdout PIPE stderr STDOUT shell True cwd bin_directory bufsize 0 universal_newlines True while True line p stdout readline self normalOutputWritten str line self te repaint if not line break\n",
      " \n",
      "\n",
      "Original Output:\n",
      " opens a file dialog and decodes the file as message.txt\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the path of the given path\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 9731 =================\n",
      "\n",
      "Original Input:\n",
      " def create_missing_partitions delay_between_attempts 60 60 log info Starting to create missing partitions while True log info Creating missing partitions db connect_to_db db query SELECT create_missing_partitions db disconnect log info Created missing partitions sleeping for d seconds delay_between_attempts time sleep delay_between_attempts\n",
      " \n",
      "\n",
      "Original Output:\n",
      " call postgresql function which creates missing table partitions ( if any ) .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " create a new instance of the given data\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 43957 =================\n",
      "\n",
      "Original Input:\n",
      " def pplot_one_output_full ax yt yv ytest ytpred yvpred ytestpred output_col ub None r2_train round r2_score yt output_col ytpred output_col 4 r2_valid round r2_score yv output_col yvpred output_col 4 r2_test round r2_score ytest output_col ytestpred output_col 4 if ub is None ub ytpred shape 0 ax output_col plot np min yt ub output_col np max yt ub output_col np min yt ub output_col np max yt ub output_col c black ax output_col scatter ytpred ub output_col flatten yt ub output_col flatten s 0 1 alpha 0 16 c b marker x label train ax output_col scatter yvpred ub output_col flatten yv ub output_col flatten s 0 1 alpha 0 16 c g marker x label valid ax output_col scatter ytestpred ub output_col flatten ytest ub output_col flatten s 0 1 alpha 0 16 c r marker x label test ax output_col set_title s r 2 s s s Y_COLUMNS output_col r2_train r2_valid r2_test return None\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"make a parity plot for the training ( blue ) , validation(green ) , and test ( red ) set predictions of a specific output measure within the full network ( trained on all the output measures at once ) .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test the given function\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "import pandas as pd \n",
    "\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=enc_pp,\n",
    "                                       decoder_preprocessor=dec_pp,\n",
    "                                       seq2seq_model=seq2seq_Model)\n",
    "\n",
    "demo_testdf = pd.DataFrame({'code':test_function_minimized, 'comment':test_docstring_minimized, 'ref':''})\n",
    "seq2seq_inf.demo_model_predictions(n=15, df=demo_testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model on disk <a name=\"save_model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq2seq_Model.save('./data/seq2seq/python_function_description_seq2seq_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for improvement <a name=\"suggestion\"></a>\n",
    "\n",
    "As can be seen from the training session, the loss values have not reached the convergent point yet. The BLEU score is also less than 0.05 which indicates a poor performance of the model. Here are several approaches we can try to improve the model in the future:\n",
    "\n",
    "**1. Use the complete training set**\n",
    "Due to the computational limitation, we only use a quarter of the training set in this project. If more cores are provided and without time constraint, we can train this model on the complete dataset which contains about 1.2 million units. This method will definitely help to increase the accuracy of the model.\n",
    "\n",
    "**2. Increase the number of epochs**\n",
    "From the training output window, it can be seen that the loss values have reduced but not converged yet. We can train the model with a higher number of epochs to get a lower loss value. Of course, this will take more time to run the training session.\n",
    "\n",
    "**3. Include the attention mechanism**\n",
    "The model in this project uses a quite simple architecture with only one layer for the encoder and decoder. It can be improved by adding one more attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
